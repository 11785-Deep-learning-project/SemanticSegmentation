{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYPntg7xu0Y1"
   },
   "source": [
    "2, 9, 10  rahul\n",
    "3, 4 shailee\n",
    "5, 6 janvi\n",
    "7, 8 gagan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJTHKKxtQh2v"
   },
   "source": [
    "Google Scholars - most cited papers on semantic segmentation are given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iXXNNsiuRta"
   },
   "source": [
    "https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf?spm=5176.100239.blogcont55892.8.pm8zm1&file=Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_011.pdf\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf\n",
    "\n",
    "https://link.springer.com/content/pdf/10.1007%2F978-3-642-33786-4_32.pdf\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf\n",
    "\n",
    "http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6247739\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Mottaghi_The_Role_of_2014_CVPR_paper.pdf\n",
    "\n",
    "http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248077\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S14-02.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1301.3572.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZsrDfa2MRb7M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5xglTN1y24T"
   },
   "source": [
    "**Paper link:** https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf\n",
    "\n",
    "\n",
    "**Actual Problem addressed:**\n",
    "\n",
    "Semantic segmentation based on FCNs have a couple of critical limitations. First, the network has a predefined fixed-size receptive field. Therefore, the object that is substantially larger or smaller than the receptive field may be fragmented or mislabeled. In other words, label prediction is done with only local information for large objects and the pixels that belong to the same object may have inconsistent labels as shown in Figure 1(a)(in the link given below). Also, small objects are often ignored and classified as background, which is illustrated in Figure 1(b)(in the link given below). Although some approaches attempt to sidestep this limitation using skip architecture, this is not a fundamental solution since there is inherent trade-off between boundary details and semantics. Second, the detailed structures of an object are often lost or smoothed because the label map, input to the deconvolutional layer, is too coarse and deconvolution procedure is overly simple. \n",
    "\n",
    "These problem of semantic segmentation are addressed here. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction.\n",
    "\n",
    "\n",
    "**How its addressed:**\n",
    "\n",
    "To overcome such limitations, they employ a completely different strategy to perform semantic segmentation based on CNN. Their main contributions are summarized below: \n",
    "* They learn a deep deconvolution network, which is composed of deconvolution, unpooling, and rectified linear unit (ReLU) layers. Learning deep deconvolution networks for semantic segmentation is meaningful but no one had attempted to do it. \n",
    "* The trained network is applied to individual object proposals to obtain instance-wise segmentations, which are combined for the final semantic segmentation; it is free from scale issues found in the original FCN-based methods and identifies finer details of an object. \n",
    "* They achieve outstanding performance using the deconvolution network trained on PASCAL VOC 2012 augmented dataset, and obtain the best accuracy through the ensemble with FCN by exploiting the heterogeneous and complementary characteristics of our algorithm with respect to FCN-based methods. \n",
    "Contrary to simple deconvolution performed on coarse activation maps, the algorithm in this paper generates object segmentation masks using deep deconvolution network, where a dense pixel-wise class probability map is obtained by successive operations of unpooling, deconvolution, and rectification. \n",
    "\n",
    "\n",
    "**Dataset used:** PASCAL VOC 2012 \n",
    "\n",
    "\n",
    "**How performance is evaluated: **\n",
    "\n",
    "They evaluated their network on PASCAL VOC 2012 segmentation benchmark, which contains 1456 test images and involves 20 object categories. They adopted the comp6 evaluation protocol that measures scores based on Intersection over Union (IoU) between ground truth and predicted segmentations. \n",
    "\n",
    "\n",
    "**Link to images: **https://docs.google.com/document/d/1DegqtK9kHFYJFSihEH-CbQi5GLk84dWVDg3sNGmwBVo/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "B27RG1gQRTq2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtIjXw1_13FZ"
   },
   "source": [
    "**Describing the Scene as a Whole: Joint Object Detection, Scene Classificationand Semantic Segmentation**\n",
    "\n",
    "**Paper Link **: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6247739\n",
    "\n",
    "This paper adresses the problem of scene understanding. It thus classifies the scene type i.e. city, sea, etc as well as the presence of various objects in those scenes which means car, boat, building dog, etc. They use a message passing scheme to transfer information to make the inference possible. The inference is done using the conditional random fields CRFs that contain a segment and a supersegment layer. The segments and super-segments show class labels to be assigned to each pixel in the image. After these segments, there is a binary layer that detects if the predicted class of the segments(bounding box created by the segments) is correct or wrong and to accept the decision or no. The next layers detect the class labels for various objects and then the scene types. There are various pairwise potentials created so that the inference can be made. \n",
    "For example, \n",
    "-including a shape prior into the model, that contains the shapes of the objects under consideration and hence any pixel lying under the bounding box reated by this prior can be detected as that class.  \n",
    "-scene class probability which means that the detection of the object type may depend on the scene type. Hence, if it is a scene of a city, then the object car has more probability than a boat.\n",
    "The loss function then has various losses, at the segmentation layers, loss is calculated as the number of misclassified pixels. A 0-1 loss function is used at the object type and scene type layers. A PASCAL loss is used over the detections which decomposes as the sum of losses for each detection.\n",
    "This model has been applied on the MSRC-21 dataset for semantic segmentation as well as on the PASCAL VOC2010 benchmark for object segmentation.\n",
    "They validated the model by comparing it with the Model created earlier (mentioned in the paper) which was cleaner. They used the standard error measure of average per class accuracy as well as average per-pixel accuracy as the global accuracy measure. They outperformed segmentation accuracy at that time as compared to the other models. They used one of the methods that used LSVM with and without context(refered as original detector) to test detection of their model.  They used a recall score and average precision to validate the detection probabilities with various models. The training got an accuarcy of around 79.3% in 3.5 hours and the inference take 7.3 seconds per image to get full inference accuarcy. Some of the classes were entirely labeled incorrecty as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XvaTEctg2FMz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9b_IRMkQOmK1"
   },
   "source": [
    "**Fully Convolutional Networks for Semantic Segmentation**\n",
    "\n",
    "**Paper Link:** https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_011.pdf\n",
    "\n",
    "**Problem Addressed:** \n",
    "\n",
    "\n",
    "1.   To show that a fully convolutional network (FCN) trained end-to-end, pixels-to-pixels on semantic segmentation exceeds the state-of-the-art without further machinery back then.\n",
    "\n",
    "2.   To make the process efficient by enabling  pixel-wise prediction during upsampling layers and learning in nets with subsampled pooling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Proposed Solution:**\n",
    "\n",
    "First use of fully convolutional neural networks to perform end-to-end segmentation of natural images.\n",
    "\n",
    "A few key features of this solutions are:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.  The features are merged from different stages in the encoder which vary in coarseness of semantic information.\n",
    "2.  The upsampling of learned low resolution semantic feature maps is done using deconvolutions which are initialized with bilinear interpolation filters, basically upsampling is backwards strided convolution.\n",
    "\n",
    "1.  Excellent example for knowledge transfer from modern classifier networks like VGG16, Alexnet to perform semantic segmentation\n",
    "\n",
    "\n",
    "**Training Method :**\n",
    "\n",
    "\n",
    "\n",
    "> **Loss:**\n",
    "  Per-pixel, unnormalized softmax loss, the softmax operation induces competition between classes and promotes the most confident prediction. For comparison, training was done with the sigmoid cross- entropy loss and it gave similar results.\n",
    "\n",
    "\n",
    "\n",
    ">**Optimizer and Learning rate:** SGD with momentum 0.9 with LR 10e-3, 10e-5 and 5e-5  for AlexNet,VGG-16 and GoogleNet respectively\n",
    "\n",
    "\n",
    "\n",
    ">**Batch size:** 20 images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Novelty introduced :**\n",
    "\n",
    "\n",
    "1.   No need for patchwise training, faster and similar performance\n",
    "\n",
    "2.   Be able to combine coarse, semantic information and shallow, fine information\n",
    "\n",
    "3.   Introduce skip connections to improve over the coarseness of upsampling\n",
    "\n",
    "\n",
    "**Dataset:**\n",
    "PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012)\n",
    "\n",
    "**Validation:** \n",
    "Training with a per-pixel multinomial logistic loss and validation with the standard metric of mean pixel intersection over union, with the mean taken over all classes, including background. The training ignores pixels that are masked out (as ambiguous or difficult) in the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DG-Mk58lRWBX"
   },
   "source": [
    "### Semantic Segmentation using Regions and Parts\n",
    "__Paper link__: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248077\n",
    "\n",
    "#### Problem Statement:\n",
    "They focus mainly on segmentation of objects particularly humans and animals. The problem with such dataset is that at one hand they have characteristic global shape but tend to exhibit large intra-class variations. On the other hand they appear in different poses.To address the first problem, they use region-based approaches that use semantic segmentation and for the second one they use multiscale scanning window detectors and part detectors.\n",
    "\n",
    "#### DataSet: \n",
    "PASCAL VOC 2012\n",
    "\n",
    "#### Pipeline:\n",
    "Generation of Candidates ( Region Generation ) --> Region representation --> Detector design --> Combination\n",
    "\n",
    "#### Experiment and Evaluation:\n",
    "Instead of relying on the coherence of appearance of each object, they combine the outputs of general top-down detectors.\n",
    "In simple words, they obtain regions in a bottom manner and use top down cues from object detectors to score and classify regions.\n",
    "They evaluate the generated region based on how accurately they are representing the ground truth mask for large majority of objects.(More on ground truth can be found here https://www.mathworks.com/matlabcentral/answers/25220-what-is-ground-truth-image-how-to-create-it-in-matlab?requestedDomain=true ) They represent regions using multiclass features. However, the highlight of their pipeline is pixel level classification. After the regions are scored their goal is to make pixel level decision. But this is a bigger problem because a single pixel may be part of several different regions \n",
    "and hence may have several different scores.\n",
    "\n",
    "#### Their pixel level classification approach:\n",
    "So this is the place where they incorporate semantic segmentation techniques:\n",
    "They train the final set of classifiers that operate on pixels rather than on regions. \n",
    "This is how they generate feature vectors per pixel considering the following ways of projecting region scores onto pixels:\n",
    "1. Each pixel receives the average score of all the regions it is part of.\n",
    "2. Each pixel receives the maximum score among all the regions it is part of.\n",
    "3. They do non-max suppression on the regions, choosing the highest scoring region, then discard all overlapping regions, and repeat. Each pixel then receives the score of the highest scoring region.\n",
    "\n",
    "With their experimentation they observe significant performance improvement on PASCAL dataset with their proposed pixel level classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Role of Context for Object Detection and Semantic Segmentation in the Wild\n",
    "__Paper Link:__ https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Mottaghi_The_Role_of_2014_CVPR_paper.pdf\n",
    "\n",
    "#### Problem Statement:\n",
    "Their goal is to analyse the effect of contexual information in detecting and segmenting images. The intuition behind this is that humans are worse than machines at classifying small image patches but are far better when more contextual information is available.They label each pixel of the training set of PASCAL to analyse the same.\n",
    "\n",
    "#### Dataset:\n",
    "They have built a contexual dataset out of PASCAL with pixel-wise labels and they categorise the whole dataset as objects, stuffs and hybrids. Objects are well defined by shapes, Best example: classes such as fork,cup etc.\n",
    "Stuffs are the ones that aren't well defined by shapes, for example: sky, grass etc. Hybrids are those with\n",
    "shaped being so variable that they cannot be easily modeled.\n",
    "They have spent considerable amount of time on annotating the data and have used an annotating model similar to LabelMe. They call it PASCAL-CONTEXT dataset.\n",
    "\n",
    "#### Their Proposed Model:\n",
    "They improve the popular Deformable Part based (DPM) contextual model with additional random variables denoting\n",
    "contextual parts, also deformable, which score the “contextual classes” around the object. Along with this, they use the global context by scoring context classes present in the full image.\n",
    "\n",
    "They use semantic segmentation to compute features for the contextual parts. Their basic idea is to count\n",
    "pixels belonging to each contextual class inside each context part’s box, and then normalize that by the area of the part’s box. concatenating the normalized counts for all context classes, they get the segmentation feature for each context part.\n",
    "For this purpose, they analyse the following segmentation algorithms.\n",
    "\n",
    "##### State-of-the-art Segmentation Algorithms:\n",
    "\n",
    "__SuperParsing:__ performs scene-level matching with the training set followed by superpixel matching. It then\n",
    "employs an MRF to incorporate neighboring contextual information. But because of the high variability of PASCAL images the algorithm performs poorly.\n",
    "\n",
    "__O2P:__  Uses shape-informed features to predict the amount of region’s overlap with a Ground truth segment for each\n",
    "class. The original method work with bottom-up region proposals which are trained to detect object-like regions. But in this paper based on their dataset (which has more of hybrid and stuffs classes) they tweak it a bit by computing UCM superpixels and then learn a classifier on the superpixels to predict their class using features based on SIFT, colorSIFT, and LBP. \n",
    "(more on UCM superpixels here: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.123.9972&rep=rep1&type=pdf\n",
    " more on SIFT and LBP here: https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\n",
    " https://en.wikipedia.org/wiki/Local_binary_patterns\n",
    " )\n",
    " \n",
    "Finally, for global context, they use a binary feature for each class, where 1 indicates that at least 1000 pixels were labeled with this class in the segmentation output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Outcomes:\n",
    "1. Nearest neighbouring methods donot generalize well.\n",
    "2. They observe a very good recall with O2P  \n",
    "3. Small objects are detected using surrounding context pretty well. For example, a boat far away from the camera position is a small object with \n",
    "small number of pixels representing it. However, the boat can be detected using the cue given by the large mass of water surrounding it.\n",
    "4. DPM rescoring has very good improvement for extra-large objects but not so useful for very small objects.\n",
    "5. They classify based on two aspects. Global context and local context and global context to determine the type of scene. This is intuitive in the sense that, once they find that the global context of an image is say sky, they more likely detect planes/birds rather than a TV.\n",
    "\n",
    "#### Evaluation Metrics used:\n",
    "Recall: Percentage of correctly labeled pixels\n",
    "Average Precision associated with PASCAL 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmYZ_W4oRW2E"
   },
   "source": [
    "#### What can we take out of these two papers?\n",
    "\n",
    "1. __About Dataset:__ \n",
    "Both the papers use PASCAL Visual Object Challenge. Also many other sources quote PASCAL VOC 2012 as the most important dataset for semantic segmentation One of the recent works on PASCAL can be found here http://cvgl.stanford.edu/projects/pascal3d.html. \n",
    "This dataset provides room for 3D object detection and pose estimation.They enhance the PASCAL VOC 2012 dataset with 3D annotations. They call it PASCAL 3D+ and also provide a new metric AVP.\n",
    "The link as very good description. I strongly feel we can use this as our dataset if everything goes fine. Also, https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Mottaghi_The_Role_of_2014_CVPR_paper.pdf\n",
    "have worked on pixel wise labeling for PASCAL VOC 2010 dataset.Though the dataset is older than the 2012 or the PASCAL 3D+,\n",
    "the pixel wise labels could be of good help for our project.\n",
    "\n",
    "2. __About Usecase:__\n",
    "The paper http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248077 stresses on 3D pose estimation and 3D object detection. This could be taken to consideration as our use case.\n",
    "\n",
    "3. __Pixel Level Classification:__\n",
    "This as mentioned in http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248077 seems interesting. Reading the following paper to understand more on this.\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pinheiro_From_Image-Level_to_2015_CVPR_paper.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Deep Learning Project.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
